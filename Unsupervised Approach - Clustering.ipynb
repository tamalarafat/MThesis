{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All standard imports for the task\n",
    "Here you can find all the modules I have used for the task. You can find therse modules where I have applied mentioned as comment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "plt.rc('font', size = 11)\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette(sns.color_palette('Paired'))\n",
    "sns.palplot(sns.color_palette('Paired'))\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the raw dataset\n",
    "As the raw data does not hold separate label information and for each class there are 3 replicates, so the labeling of the class was manully done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "dataset_read = pd.read_excel('pilot_experiment_TPM_WTonly.xlsx') #reading the dataset using pandas read_excel function\n",
    "unprocessed_dataset = dataset_read.iloc[:, 0:].T #Transpose of the raw dataset\n",
    "y = dataset_read.columns.values.tolist() #getting the label/target labels as list from the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_categorized = [] \n",
    "sample_list = []\n",
    "#assigning categorical label to 0-9 (ascending order) to y_categorized as label for each sample\n",
    "for i in y:\n",
    "    if i[:-2] == 'Ox_Leaf1_T1':\n",
    "        y_categorized.append(0)\n",
    "        sample_list.append('L1T1')\n",
    "    elif i[:-2] == 'Ox_Leaf1_T2':\n",
    "        y_categorized.append(1)\n",
    "        sample_list.append('L1T2')\n",
    "    elif i[:-2] == 'Ox_Leaf1_T3':\n",
    "        y_categorized.append(2)\n",
    "        sample_list.append('L1T3')\n",
    "    elif i[:-2] == 'Ox_Leaf1_T4':\n",
    "        y_categorized.append(3)\n",
    "        sample_list.append('L1T4')\n",
    "    elif i[:-2] == 'Ox_Leaf3_T2':\n",
    "        y_categorized.append(4)\n",
    "        sample_list.append('L3T2')\n",
    "    elif i[:-2] == 'Ox_Leaf3_T3':\n",
    "        y_categorized.append(5)\n",
    "        sample_list.append('L3T3')\n",
    "    elif i[:-2] == 'Ox_Leaf3_T4':\n",
    "        y_categorized.append(6)\n",
    "        sample_list.append('L3T4')\n",
    "    elif i[:-2] == 'Ox_Leaf5_T3':\n",
    "        y_categorized.append(7)\n",
    "        sample_list.append('L5T3')\n",
    "    elif i[:-2] == 'Ox_Leaf5_T4':\n",
    "        y_categorized.append(8)\n",
    "        sample_list.append('L5T4')\n",
    "    elif i[:-2] == 'Ox_Leaf7_T4':\n",
    "        y_categorized.append(9)\n",
    "        sample_list.append('L7T4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_dataset['y'] = y_categorized #putting the label information with the dataset, as the dataset does not contain the label\n",
    "dataset = unprocessed_dataset\n",
    "X = dataset.iloc[:, :-1].values #getting the dataset without label, where each row represents sample, each column represents featues or independent variables\n",
    "y = dataset.iloc[:, -1].values #label column from the data set\n",
    "\n",
    "#for visualizing purpose(for sample labeling)\n",
    "#import numpy as np\n",
    "sample_arr = np.array(sample_list)\n",
    "set_sample = set(sample_list)\n",
    "set_sample = sorted(list(set_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizytion of the data\n",
    "For the visualization I have used PCA to reduce the dimension of the raw data to two dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.decomposition import PCA\n",
    "pca_visualization = PCA(n_components = 2)\n",
    "X_transformed = pca_visualization.fit_transform(X) #transforming my features to 2 dimension\n",
    "plt.figure(figsize = (14,10))\n",
    "plt.scatter(X_transformed[:,0], X_transformed[:,1], c = y, s = 50, cmap = 'tab10', alpha = 0.7)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.colorbar()\n",
    "plt.title('Each color represent each sample set')\n",
    "#plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Report/Data.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal number of clusters\n",
    "For this task, we know how many classes we have for our data. In the case when we dont have class labels, there are methods to selcet optimal number of clusters. These methods are \n",
    "- Elbow Method\n",
    "- Based on Dendogram\n",
    "- Silhouette Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method\n",
    "The Elbow method is a method of interpretation and validation of consistency within cluster (WCSS) analysis designed to help finding the appropriate number of clusters in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1,11):\n",
    "    km_elbow = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 100)\n",
    "    km_elbow.fit(X)\n",
    "    #Inertia, or the within-cluster sum of squares criterion, can be recognized as a measure of how internally coherent \n",
    "    #clusters are.\n",
    "    wcss.append(km_elbow.inertia_)\n",
    "\n",
    "plt.figure(figsize = (14,10))\n",
    "plt.plot(range(1,11), wcss)\n",
    "plt.plot([4],wcss[3],marker = 'o', markersize = 5, color = 'red', label = 'Maximum Variance')\n",
    "plt.title('Elbow method')\n",
    "plt.xlabel('No. of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Report/Elbow_method_RawData.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Analysis(S. A.)\n",
    "Silhouette Analysis(S.A.) is a way to measure how close each point in a cluster is to the points in its neighboring clusters. Its a way to find out the optimum value for k during k-means clustering. Silhouette values lies in the range of [-1, 1]. Value of +1 is ideal and -1 is least preferred. Higher the value better is the cluster configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "avg_sil_score = []\n",
    "for i in range(2,11):\n",
    "    kmeans_sil = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 100)\n",
    "    y_kmeans_sil = kmeans_sil.fit_predict(X)\n",
    "    centers = kmeans_sil.cluster_centers_\n",
    "\n",
    "    score = silhouette_score(X, y_kmeans_sil)\n",
    "    sample_silhouette_values = silhouette_samples(X, y_kmeans_sil)\n",
    "    print(\"For n_clusters = {}, silhouette score is {})\".format(i, score))\n",
    "    avg_sil_score.append((score))\n",
    "    \n",
    "print(\"For {}, silhouette score is {})\".format('2 Clusters', avg_sil_score[0]))\n",
    "print(\"For {}, silhouette score is {})\".format('4 Clusters', avg_sil_score[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendogram(for Heirarchical clustering)\n",
    "The way that heirarchical clustering works is, it maintains a memory about how the clustering was done and that memory stored in a dendogram.\n",
    "The height of the line connected two observations or two clusters is the euclidean distance between them and also represents the computed dissimilarity between them.\n",
    "We can set dissimilarity or distance threshold, that tells us that the samples within these clusters are less dissimilar than this threshold. And the way to choose the optimal cluster is selecting the longest vertical line that intersects any extended horizontal lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import scipy.cluster.hierarchy as sch\n",
    "plt.figure(figsize = (14,10))\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'), labels = sample_list)\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Report/Dendogram_RawData.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing clustering algorithm\n",
    "We have used the unsupervised clustering approaches where the number of clusters can be predefined as we know the class label.\n",
    "- KMeans clustering\n",
    "- Agglomorative clustering\n",
    "- Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with the optimal number of clusters(K) found from the above mentioned methods\n",
    "If we dont have the label information, 2 or 4 clusters can be the optimal choice. But for this task we know the label information and we will work with 10 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_opc = KMeans(n_clusters = 4, init = 'k-means++', max_iter = 300, n_init = 100)\n",
    "y_km_opc = km_opc.fit_predict(X)\n",
    "\n",
    "plt.rc('font', size = 10)\n",
    "fig, ax = plt.subplots(figsize = (14,10))\n",
    "plt.scatter(X_transformed[:,0], X_transformed[:,1], c = y_km_opc, s = 200, cmap = plt.cm.get_cmap('viridis', 4), alpha = 0.7)\n",
    "for i, txt in enumerate(sample_list):\n",
    "    ax.annotate(txt, (X_transformed[:,0][i], X_transformed[:,1][i]))\n",
    "plt.colorbar(ticks = range(4), label = 'clusters')\n",
    "plt.clim(0, 3)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Report/Four_Clusters_RawData.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans\n",
    "k-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset. It accomplishes this using a simple conception of what the optimal clustering looks like:\n",
    "- The \"cluster center\" is the arithmetic mean of all the points belonging to the cluster.\n",
    "- Each point is closer to its own cluster center than to other cluster centers.\n",
    "\n",
    "These two assumptions are the basis of the k-means model. There are 6 steps:\n",
    "1) Randomly select k cluster centers.\n",
    "2) Calculate the distance between each data point and cluster centers.\n",
    "3) Assign the data point to the cluster center whose distance from the cluster center is minimum of all the cluster centers.\n",
    "4) Recalculate the new cluster center.\n",
    "5) Recalculate the distance between each data point and new obtained cluster centers.\n",
    "6) If no data point was reassigned then stop, otherwise repeat from step 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.cluster import KMeans\n",
    "#import matplotlib.pyplot as plt\n",
    "Kmeans = KMeans(n_clusters = 10, init = 'k-means++', max_iter = 300, n_init = 100) #Creating the Kmeans algorith object\n",
    "y_kmeans = Kmeans.fit_predict(X) #fitting the learning model to the data and predicting the clusters for the samples\n",
    "plt.rc('font', size = 10) #setting the front size in the plot\n",
    "fig, ax = plt.subplots(figsize = (14,10))\n",
    "plt.scatter(X_transformed[:,0], X_transformed[:,1], c = y_kmeans, s = 200, cmap = plt.get_cmap('tab10', 10), alpha = 0.8)\n",
    "for i, txt in enumerate(sample_list):\n",
    "    ax.annotate(txt, (X_transformed[:,0][i], X_transformed[:,1][i]))\n",
    "plt.colorbar(ticks = range(10), label = 'clusters')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Report/Rawdata_Kmeans.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering performance evaluation\n",
    "Evaluating the performance of a clustering algorithm is not as trivial as counting the number of errors or the precision and recall of a supervised classification algorithm. In particular any evaluation metric should not take the absolute values of the cluster labels into account but rather if this clustering define separations of the data similar to some ground truth set of classes or satisfying some assumption such that members belong to the same class are more similar that members of different classes according to some similarity metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the learned Kmeans model by comparing the clustered labels with our actual label y from the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.stats import mode\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#import numpy as np\n",
    "labels = np.zeros_like(y_kmeans)\n",
    "for i in range(10):\n",
    "    mask = (y_kmeans == i)\n",
    "    labels[mask] = mode(y[mask])[0]\n",
    "\n",
    "acc_score = accuracy_score(y, labels)\n",
    "print('The accuracy score for optimized K-means algorithm {}.'.format(acc_score))\n",
    "mat = confusion_matrix(y, labels)\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.heatmap(mat.T, square = False, annot = True, fmt = 'd', cbar = False, xticklabels = set_sample, yticklabels = set_sample)\n",
    "plt.xlabel('True label')\n",
    "plt.ylabel('Predicted label')\n",
    "plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Kmeans/ConfusionMatrix_RawData.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The confusion matrix does not necessarily gives the right accuarcy measure.\n",
    "If you look at L3_T2 samples, 2 samples were clustered in one cluster and 1 sample was clustered as separate cluster. But in the confusion matrix L3_T2 were shown as these 3 samples were correctly clustered. And the´L7_T4 2 samples were clustered in the L3_T2 cluster. And L3_T3 and L5_T4 were also clustered in the same cluster.\n",
    "So for unsupervised method based on unlabeled data, the evaluation metrics does not always perform well. As for our data, we have less samples(30). So it is possible to manually evaluate the Clustering method performance by visualizing the clustering result with reduced dimension by labeling each sample based on their clustered label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "#the Adjusted Rand index is a function that measures the similarity of the two assignments, \n",
    "#ignoring permutations and with chance normalization.\n",
    "#Perfect labeling is scored 1.0, Bad labeling have negative or close to 0.0 scores.\n",
    "adjusted_rand_score = metrics.adjusted_rand_score(y, y_kmeans)\n",
    "print('Adjusted rand score {}.'.format(adjusted_rand_score))\n",
    "\n",
    "#the Mutual Information is a function that measures the agreement of the two assignments. \n",
    "#Two different normalized versions of this measure are available, Normalized Mutual Information (NMI) and \n",
    "#Adjusted Mutual Information (AMI). NMI is often used in the literature, while AMI was proposed more recently \n",
    "#and is normalized against chance\n",
    "adjusted_mutual_info = metrics.adjusted_mutual_info_score(y, y_kmeans)\n",
    "print('Adjusted mutual info score {}.'.format(adjusted_mutual_info))\n",
    "\n",
    "normalized_mutual_info = metrics.normalized_mutual_info_score(y, y_kmeans)\n",
    "print('Normalized mutual info score {}.'.format(normalized_mutual_info))\n",
    "print('AMI and NMI values close to zero indicate two label assignments that are largely independent, while values close to one indicate significant agreement. Further, an AMI of exactly 1 indicates that the two label assignments are equal ')\n",
    "\n",
    "#Homogeneity, completeness and V-measure\n",
    "#homogeneity: each cluster contains only members of a single class.\n",
    "#completeness: all members of a given class are assigned to the same cluster.\n",
    "homogeneity_score, completeness_score, V_score = metrics.homogeneity_completeness_v_measure(y, y_kmeans)\n",
    "print('Homogeneity score {}.'.format(homogeneity_score))\n",
    "print('Completeness info score {}.'.format(completeness_score))\n",
    "print('V-measure score {}.'.format(V_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  For smaller sample sizes or larger number of clusters it is safer to use an adjusted index such as the Adjusted Rand Index (ARI).\n",
    "For the following tasks we will use Adjusted_rand_score, Homogeneity and completeness score as the performance evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heiarchical Clustering\n",
    "The AgglomerativeClustering object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy:\n",
    "- Ward: minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hc = AgglomerativeClustering(n_clusters = 10, affinity = 'euclidean', linkage = 'ward')\n",
    "y_hc = hc.fit_predict(X)\n",
    "\n",
    "plt.rc('font', size = 10) #setting the front size in the plot\n",
    "fig, ax = plt.subplots(figsize = (14,10)) #figure size\n",
    "plt.scatter(X_transformed[:,0], X_transformed[:,1], c = y_hc, s = 200, cmap = plt.get_cmap('tab10', 10), alpha = 0.8)\n",
    "for i, txt in enumerate(sample_list):\n",
    "    ax.annotate(txt, (X_transformed[:,0][i], X_transformed[:,1][i]))\n",
    "plt.colorbar(ticks = range(10), label = 'clusters')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Report/Rawdata_Heiarchical_Clustering.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Clustering\n",
    "It uses the graph of nearest neighbors to compute a higher-dimensional representation of the data, and then assigns labels using a k-means algorithm.\n",
    "In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.cluster import SpectralClustering\n",
    "SCluster = SpectralClustering(n_clusters = 10, n_init = 300, affinity = 'rbf', assign_labels = 'discretize')\n",
    "y_SC = SCluster.fit_predict(X)\n",
    "\n",
    "plt.rc('font', size = 10) #setting the front size in the plot\n",
    "fig, ax = plt.subplots(figsize = (14,10)) #figure size\n",
    "plt.scatter(X_transformed[:,0], X_transformed[:,1], c = y_SC, s = 200, cmap = plt.get_cmap('tab10', 10), alpha = 0.8)\n",
    "for i, txt in enumerate(sample_list):\n",
    "    ax.annotate(txt, (X_transformed[:,0][i], X_transformed[:,1][i]))\n",
    "plt.colorbar(ticks = range(10), label = 'clusters')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Report/Rawdata_Spectral_Clustering.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans and Heirarchical Clustering produce the same result and both of the clustering approaches almost work in the same way. As for spectral clustering, the output is not desirable. As our task for unsupervised approach is to find if there is any pattern in the data, for the following task we will be using KMeans clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying KMeans in scaled features(Normalized)\n",
    "The dataset contains features highly varying in magnitudes and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations, this is a problem.The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes.\n",
    "To supress this effect, we need to bring all features to the same level of magnitudes. This can be acheived by scaling.\n",
    "Standardisation can be used for algorithms that assumes zero centric data like Principal Component Analysis(PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc_X_train = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(.9999) #retaining 99.99 variaance of the data\n",
    "X_trans = pca.fit_transform(sc_X_train)\n",
    "num_components = pca.n_components_\n",
    "print('{} components retain 99.99% variance of the data and shape {}.'.format(num_components, X_trans.shape))\n",
    "\n",
    "plt.figure(figsize = (14,10))\n",
    "plt.scatter(X_trans[:,0], X_trans[:,1], c = y, s = 50, cmap = 'tab10', alpha = 0.7)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.colorbar()\n",
    "plt.title('Normalized raw data Visualization')\n",
    "plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Report/rawData_Normalized.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans on normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_nr = KMeans(n_clusters = 10, init = 'k-means++', max_iter = 300, n_init = 100)\n",
    "y_km_nr = km_nr.fit_predict(sc_X_train)\n",
    "\n",
    "plt.rc('font', size = 10) #setting the front size in the plot\n",
    "fig, ax = plt.subplots(figsize = (14,10))\n",
    "plt.scatter(X_trans[:,0], X_trans[:,1], c = y_km_nr, s = 200, cmap = plt.get_cmap('tab10', 10), alpha = 0.8)\n",
    "for i, txt in enumerate(sample_list):\n",
    "    ax.annotate(txt, (X_trans[:,0][i], X_trans[:,1][i]))\n",
    "plt.colorbar(ticks = range(10), label = 'clusters')\n",
    "plt.title('KMeans on the normalized data')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Report/Kmeans_normalized_Rawdata.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the Kmeans on normalized data\n",
    "- Adjusted random score\n",
    "- Homogeneity score\n",
    "- Completeness score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_rand_score = metrics.adjusted_rand_score(y, y_km_nr)\n",
    "print('Adjusted rand score {}.'.format(adjusted_rand_score))\n",
    "\n",
    "homogeneity_score, completeness_score, V_score = metrics.homogeneity_completeness_v_measure(y, y_km_nr)\n",
    "print('Homogeneity score {}.'.format(homogeneity_score))\n",
    "print('Completeness info score {}.'.format(completeness_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that doing feature scaling or Standardization improve the accuracy of the KMeans clustering output. So for the following task feature normalization is preffered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying PCA to reduce the dimensionality\n",
    "- Linear dimensionality reduction using PCA\n",
    "- Non-linear dimensionality reduction using KernelPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying PCA on RawData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_rawdata = PCA(.9999)\n",
    "X_transform_rd = pca_rawdata.fit_transform(X)\n",
    "num_components = pca_rawdata.n_components_\n",
    "print('{} components retain 99.99% variance of the data and shape {}.'.format(num_components, X_transform_rd.shape))\n",
    "\n",
    "km_pca_rd = KMeans(n_clusters = 10, init = 'k-means++', max_iter = 300, n_init = 100)\n",
    "y_pca_rd = km_pca_rd.fit_predict(X_transform_rd)\n",
    "\n",
    "plt.rc('font', size = 10) #setting the front size in the plot\n",
    "fig, ax = plt.subplots(figsize = (14,10))\n",
    "plt.scatter(X_transform_rd[:,0], X_transform_rd[:,1], c = y_pca_rd, s = 200, cmap = plt.get_cmap('tab10', 10), alpha = 0.7)\n",
    "for i, txt in enumerate(sample_list):\n",
    "    ax.annotate(txt, (X_transform_rd[:,0][i], X_transform_rd[:,1][i]))\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.colorbar()\n",
    "plt.title('KMeans with reduced dimension of raw data')\n",
    "plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Report/rawData_PCA.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the dimension of raw data using PCA does not change the output of the cluster and does not improve the performance. So linear PCA does not improve the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KernalPCA : Non-linear dimensionality reduction through the use of kernels\n",
    "kernel trick, a method to project original data into higher dimension without sacrificing too much computational time. (Non-linear feature mapping). The basic idea to deal with linearly inseparable data is to project it onto a higher dimensional space where it becomes linearly separable.\n",
    "The “classic” PCA approach is a linear projection technique that works well if the data is linearly separable. However, in the case of linearly inseparable data, a nonlinear technique is required if the task is to reduce the dimensionality of a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KernelPCA on rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca_rd = KernelPCA(n_components = 29, kernel = 'rbf')\n",
    "X_transform_rd = kpca_rd.fit_transform(X)\n",
    "\n",
    "km_kpca_rd = KMeans(n_clusters = 10, init = 'k-means++', max_iter = 300, n_init = 100)\n",
    "y_kpca_rd = km_kpca_rd.fit_predict(X_transform_rd)\n",
    "\n",
    "plt.rc('font', size = 10) #setting the front size in the plot\n",
    "fig, ax = plt.subplots(figsize = (14,10))\n",
    "plt.scatter(X_transform_rd[:,0], X_transform_rd[:,1], c = y_kpca_rd, s = 200, cmap = plt.get_cmap('tab10', 10), alpha = 0.7)\n",
    "for i, txt in enumerate(sample_list):\n",
    "    ax.annotate(txt, (X_transform_rd[:,0][i], X_transform_rd[:,1][i]))\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.colorbar(label = 'Clusters')\n",
    "plt.title('KMeans with reduced dimension of raw data with KPCA')\n",
    "plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Report/rawData_KPCA.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can come to the conclusion that either linear or kernel PCA do not perform well on the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying linear PCA on normalized data using StandardScalar(0 mean, 1 std.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_nd = PCA(.9999)\n",
    "X_transform_nd = pca_nd.fit_transform(sc_X_train)\n",
    "num_components = pca_nd.n_components_\n",
    "print('{} components retain 99.99% variance of the data and shape {}.'.format(num_components, X_transform_nd.shape))\n",
    "\n",
    "km_pca_nd = KMeans(n_clusters = 10, init = 'k-means++', max_iter = 300, n_init = 100)\n",
    "y_pca_nd = km_pca_nd.fit_predict(X_transform_nd)\n",
    "\n",
    "plt.rc('font', size = 10) #setting the front size in the plot\n",
    "fig, ax = plt.subplots(figsize = (14,10))\n",
    "plt.scatter(X_transform_nd[:,0], X_transform_nd[:,1], c = y_pca_nd, s = 200, cmap = plt.get_cmap('tab10', 10), alpha = 0.7)\n",
    "for i, txt in enumerate(sample_list):\n",
    "    ax.annotate(txt, (X_transform_nd[:,0][i], X_transform_nd[:,1][i]))\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.colorbar()\n",
    "plt.title('KMeans with reduced dimension of raw data')\n",
    "plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Report/normalizedData_PCA.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying non-linear KernelPCA on normalized data using StandardScalar(0 mean, 1 std.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca_nd = KernelPCA(n_components = 5, kernel = 'rbf')\n",
    "X_transform_nd = kpca_nd.fit_transform(sc_X_train)\n",
    "\n",
    "km_kpca_nd = KMeans(n_clusters = 10, init = 'k-means++', max_iter = 300, n_init = 100)\n",
    "y_kpca_nd = km_kpca_nd.fit_predict(X_transform_nd)\n",
    "\n",
    "plt.rc('font', size = 10) #setting the front size in the plot\n",
    "fig, ax = plt.subplots(figsize = (14,10))\n",
    "plt.scatter(X_transform_nd[:,0], X_transform_nd[:,1], c = y_kpca_nd, s = 200, cmap = plt.get_cmap('tab10', 10), alpha = 0.7)\n",
    "for i, txt in enumerate(sample_list):\n",
    "    ax.annotate(txt, (X_transform_nd[:,0][i], X_transform_nd[:,1][i]))\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.colorbar(label = 'Clusters')\n",
    "plt.title('KMeans with reduced dimension of raw data with KPCA')\n",
    "plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Report/normalisedData_KPCA.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca = KernelPCA(kernel = 'rbf')\n",
    "X_transformed = kpca.fit_transform(sc_X_train)\n",
    "\n",
    "explained_variance = np.var(X_transformed, axis=0)\n",
    "explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "pca_components_var = np.cumsum(explained_variance_ratio)\n",
    "num_components = 0\n",
    "for i in pca_components_var:\n",
    "    if i <= 0.9999:\n",
    "        num_components = num_components + 1\n",
    "    else:\n",
    "        break\n",
    "print('By using KernelPCA only 40% variance of the data kept(5 KernelPCA components) that gives better KMeans accuracy. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 5 KernelPCA components were kept from 29 components. 5 components hold 40% variance of the data, to cluster the samples of each class in their own cluster. When using all the components to keep 99.99% variance, KMeans cannot cluster the samples accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_rand_score = metrics.adjusted_rand_score(y, y_kpca_nd)\n",
    "print('Adjusted rand score for KMeans using KernelPCA {}.'.format(adjusted_rand_score))\n",
    "\n",
    "homogeneity_score, completeness_score, V_score = metrics.homogeneity_completeness_v_measure(y, y_kpca_nd)\n",
    "print('Homogeneity score for KMeans using KernelPCA {}.'.format(homogeneity_score))\n",
    "print('Completeness info score for KMeans using KernelPCA {}.'.format(completeness_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "Applying KernelPCA improves the accuracy of the KMeans clustering method. From this finding we can say:\n",
    "- The data is linearly inseparable\n",
    "- Feature Scaling has profound effect on the output and accuracy\n",
    "- By reducing the dimension by non-linear PCA gives better accuracy\n",
    "- Only 41% variance of the data gives KMeans better clustering accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating the result by using Supervised dimensionality reduction technique - LDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general LDA approach is very similar to a Principal Component Analysis, but in addition to finding the component axes that maximize the variance of our data (PCA), we are additionally interested in the axes that maximize the separation between multiple classes (LDA).\n",
    "The goal of an LDA is to project a feature space (a dataset n-dimensional samples) onto a smaller subspace k (where k≤n−1) while maintaining the class-discriminatory information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA on normalized data to reduce the dimensionality and applying KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lda = LDA(n_components = 8)\n",
    "X_transform_lda = lda.fit_transform(sc_X_train, y)\n",
    "lda.fit(X_transformed, y)\n",
    "\n",
    "Km_lda = KMeans(n_clusters = 10, init = 'k-means++', max_iter = 300, n_init = 100)\n",
    "y_lda = Km_lda.fit_predict(X_transformed)\n",
    "\n",
    "plt.rc('font', size = 10) #setting the front size in the plot\n",
    "fig, ax = plt.subplots(figsize = (14,10))\n",
    "plt.scatter(X_transform_lda[:,0], X_transform_lda[:,1], c = y_lda, s = 200, cmap = plt.get_cmap('tab10', 10), alpha = 0.7)\n",
    "for i, txt in enumerate(sample_list):\n",
    "    ax.annotate(txt, (X_transform_lda[:,0][i], X_transform_lda[:,1][i]))\n",
    "plt.xlabel('LDA C1')\n",
    "plt.ylabel('LDA C2')\n",
    "plt.colorbar()\n",
    "plt.title('KMeans with reduced dimension of normalized data by LDA')\n",
    "plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Report/normalizedData_LDA.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see, even taking the class or label information, linear separation of the data does not help to improve accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA as a supervised learning (as Classifier)\n",
    "This was done just to see how the LDA works as a classifier and showing the output with different n_components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "acc1 = []\n",
    "for i in range(1, 10):\n",
    "    lda = LDA(n_components = i)\n",
    "    X_transformed = lda.fit_transform(sc_X_train, y)\n",
    "    lda.fit(X_transformed, y)\n",
    "    y_pred = lda.predict(X_transformed)\n",
    "    acc1.append(accuracy_score(y, y_pred))\n",
    "    print('Accuracy score {} for {} LDA components.'.format(accuracy_score(y, y_pred), i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a pipeline for predicting new sample with: StandardScalar + KernelPCA + KMeans \n",
    "The following script is for test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_label(new_sample_list):\n",
    "    pipeline = (Pipeline([('Scalar', StandardScaler()), ('KernelPCA', KernelPCA(n_components = 5, kernel = 'rbf')), \n",
    "                          ('Kmeans', KMeans(n_clusters = 10, n_init = 100, max_iter = 300, init = 'k-means++'))]))\n",
    "    y_train = pipeline.fit_predict(X)\n",
    "    y_new_sample = pipeline.predict(new_sample_list)\n",
    "    \n",
    "    return y_train, y_new_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put sample list lable to visualize the label of the test sample\n",
    "test_sample_label = ['test_Sample_1', 'test_Sample_2']\n",
    "sc_ = StandardScaler()\n",
    "sc_X = sc_.fit_transform(X)\n",
    "#put new sample or sample list here\n",
    "test_sample_ = \n",
    "\n",
    "kpca_ = KernelPCA(n_components = 5, kernel = 'rbf')\n",
    "X_transform_ = kpca_.fit_transform(sc_X)\n",
    "test_sample_trans_ = kpca_.transform(test_sample_)\n",
    "\n",
    "km_kpca_ = KMeans(n_clusters = 10, init = 'k-means++', max_iter = 300, n_init = 100)\n",
    "y_train_ = km_kpca_.fit_predict(X_transform_)\n",
    "y_test_ = km_kpca_.predict(test_sample_)\n",
    "\n",
    "plt.rc('font', size = 10) #setting the front size in the plot\n",
    "fig, ax = plt.subplots(figsize = (14,10))\n",
    "plt.scatter(X_transform_[:,0], X_transform_[:,1], c = y_train_, s = 200, cmap = plt.get_cmap('tab10', 10), alpha = 0.7)\n",
    "plt.scatter(X_transform_[:,0], X_transform_[:,1], c = y_test_, s = 200, cmap = plt.get_cmap('tab10', 10), alpha = 0.7)\n",
    "\n",
    "for i, txt in enumerate(sample_list):\n",
    "    ax.annotate(txt, (X_transform_[:,0][i], X_transform_[:,1][i]))\n",
    "    \n",
    "for j, txt in enumerate(test_sample_label):\n",
    "    ax.annotate(txt, (test_sample_trans_[:,0][j], test_sample_trans_[:,1][j]))\n",
    "    \n",
    "plt.xlabel('LDA C1')\n",
    "plt.ylabel('LDA C2')\n",
    "plt.colorbar()\n",
    "plt.title('KMeans with reduced dimension of normalized data by KPCA')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
