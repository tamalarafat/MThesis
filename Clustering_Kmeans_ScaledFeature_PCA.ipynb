{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "plt.rc('font', size = 11)\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette(sns.color_palette('Paired'))\n",
    "sns.palplot(sns.color_palette('Paired'))\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "dataset_read = pd.read_excel('pilot_experiment_TPM_WTonly.xlsx') #reading the dataset using pandas read_excel function\n",
    "unprocessed_dataset = dataset_read.iloc[:, 0:].T #Transpose of the raw dataset\n",
    "y = dataset_read.columns.values.tolist() #getting the label/target labels as list from the dataset \n",
    "\n",
    "y_categorized = [] \n",
    "sample_list = []\n",
    "#assigning categorical label to 0-9 (ascending order) to y_categorized as label for each sample\n",
    "for i in y:\n",
    "    if i[:-2] == 'Ox_Leaf1_T1':\n",
    "        y_categorized.append(0)\n",
    "        sample_list.append('L1T1')\n",
    "    elif i[:-2] == 'Ox_Leaf1_T2':\n",
    "        y_categorized.append(1)\n",
    "        sample_list.append('L1T2')\n",
    "    elif i[:-2] == 'Ox_Leaf1_T3':\n",
    "        y_categorized.append(2)\n",
    "        sample_list.append('L1T3')\n",
    "    elif i[:-2] == 'Ox_Leaf1_T4':\n",
    "        y_categorized.append(3)\n",
    "        sample_list.append('L1T4')\n",
    "    elif i[:-2] == 'Ox_Leaf3_T2':\n",
    "        y_categorized.append(4)\n",
    "        sample_list.append('L3T2')\n",
    "    elif i[:-2] == 'Ox_Leaf3_T3':\n",
    "        y_categorized.append(5)\n",
    "        sample_list.append('L3T3')\n",
    "    elif i[:-2] == 'Ox_Leaf3_T4':\n",
    "        y_categorized.append(6)\n",
    "        sample_list.append('L3T4')\n",
    "    elif i[:-2] == 'Ox_Leaf5_T3':\n",
    "        y_categorized.append(7)\n",
    "        sample_list.append('L5T3')\n",
    "    elif i[:-2] == 'Ox_Leaf5_T4':\n",
    "        y_categorized.append(8)\n",
    "        sample_list.append('L5T4')\n",
    "    elif i[:-2] == 'Ox_Leaf7_T4':\n",
    "        y_categorized.append(9)\n",
    "        sample_list.append('L7T4')\n",
    "\n",
    "unprocessed_dataset['y'] = y_categorized #putting the label information with the dataset, as the dataset does not contain the label\n",
    "dataset = unprocessed_dataset\n",
    "X = dataset.iloc[:, :-1].values #getting the dataset without label, where each row represents sample, each column represents featues or independent variables\n",
    "y = dataset.iloc[:, -1].values #label column from the data set\n",
    "\n",
    "#for the plotting with the labels\n",
    "#import numpy as np\n",
    "sample_arr = np.array(sample_list)\n",
    "set_sample = set(sample_list)\n",
    "set_sample = sorted(list(set_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different feature scaling methods:\n",
    "#### Standard Scalar, MinMax Scalar, Quantile Transformer, Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandardScaler:\n",
    "removes the mean and scales the data to unit variance. However, the \n",
    "outliers have an influence when computing the empirical mean and standard deviation \n",
    "which shrink the range of the feature values as shown in the left figure below. Note in \n",
    "particular that because the outliers on each feature have different magnitudes, the \n",
    "spread of the transformed data on each feature is very different.\n",
    "StandardScaler therefore cannot guarantee balanced feature scales in the presence of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc_X_train = sc.fit_transform(X)\n",
    "print('For standard Scalar, the max value for each row is same:', np.amax(sc_X_train, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minmax Scalar:\n",
    "rescales the data set such that all feature values are in the range [0, 1]\n",
    "#As StandardScaler, MinMaxScaler is very sensitive to the presence of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "mms_X_train = mms.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QuantileTransformer\n",
    "has an additional output_distribution parameter allowing to match a \n",
    "Gaussian distribution instead of a uniform distribution. Note that this non-parametetric \n",
    "transformer introduces saturation artifacts for extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "qt = QuantileTransformer(output_distribution = 'uniform')\n",
    "qt_X_train = qt.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Normalizer\n",
    "rescales the vector for each sample to have unit norm, independently of \n",
    "the distribution of the samples. It can be seen on both figures below where all samples \n",
    "are mapped onto the unit circle. In our example the two selected features have only \n",
    "positive values; therefore the transformed data only lie in the positive quadrant. This \n",
    "would not be the case if some original features had a mix of positive and negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "nr = Normalizer()\n",
    "nr_X_train = nr.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the scaled data by reducing the dimensionality with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.decomposition import PCA\n",
    "pca_visualization = PCA(n_components = 2)\n",
    "#Using the PCA on my scaled features\n",
    "X_transformed = pca_visualization.fit_transform(sc_X_train) \n",
    "#transforming my features to 2 dimension(feature extraction from my orginal feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,10))\n",
    "plt.scatter(X_transformed[:,0], X_transformed[:,1], c = y, s = 50, cmap = 'tab10', alpha = 0.7)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.colorbar()\n",
    "plt.title('Each color represent each sample set')\n",
    "#plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Kmeans/Normalized_Quantile_PCA-transformed.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Kmeans algorithm to the scaled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.cluster import KMeans\n",
    "#import matplotlib.pyplot as plt\n",
    "Kmeans = KMeans(n_clusters = 10, init = 'k-means++', max_iter = 300, n_init = 100) #creating the Kmeans object\n",
    "Kmeans.fit(sc_X_train)\n",
    "y_kmeans = Kmeans.fit_predict(sc_X_train) #fitting the learning model to the data and predicting the clusters for the samples\n",
    "plt.rc('font', size = 10) #setting the front size in the plot\n",
    "fig, ax = plt.subplots(figsize = (14,10)) #figure size\n",
    "plt.scatter(X_transformed[y_kmeans == 0, 0], X_transformed[y_kmeans == 0, 1], s = 300, c = 'red', label = 'Cluster 1', alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_kmeans == 1, 0], X_transformed[y_kmeans == 1, 1], s = 300, c = 'blue', label = 'Cluster 2', alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_kmeans == 2, 0], X_transformed[y_kmeans == 2, 1], s = 300, c = 'green', label = 'Cluster 3', alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_kmeans == 3, 0], X_transformed[y_kmeans == 3, 1], s = 300, c = 'cyan', label = 'Cluster 4', alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_kmeans == 4, 0], X_transformed[y_kmeans == 4, 1], s = 300, c = 'silver', label = 'Cluster 5', alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_kmeans == 5, 0], X_transformed[y_kmeans == 5, 1], s = 300, c = 'peru', label = 'Cluster 6',alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_kmeans == 6, 0], X_transformed[y_kmeans == 6, 1], s = 300, c = 'lawngreen', label = 'Cluster 7', alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_kmeans == 7, 0], X_transformed[y_kmeans == 7, 1], s = 300, c = 'lightgreen', label = 'Cluster 8', alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_kmeans == 8, 0], X_transformed[y_kmeans == 8, 1], s = 300, c = 'pink', label = 'Cluster 9', alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_kmeans == 9, 0], X_transformed[y_kmeans == 9, 1], s = 300, c = 'purple', label = 'Cluster 10', alpha = 0.5)\n",
    "for i, txt in enumerate(sample_list):\n",
    "    ax.annotate(txt, (X_transformed[:,0][i], X_transformed[:,1][i]))\n",
    "plt.legend()\n",
    "#fig.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Kmeans/Kmeans_Quantile_NormalizedData.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By applying feature scaling Standard Scalar, MinMax Scalar, Quantile Transformer all the three samples from L5_T3, L7_T4 were correctly clustered which is unlikely without scaled feature. Still L3_T2 samples were clustered in 2 different clusters.And as for raw input with or without PCA dimensionality reduction and scaled features L3_T4 and L5_T4 were always clustred together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the dendrogram to find the optimal number of clusters\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "plt.figure(figsize = (14,10))\n",
    "dendrogram = sch.dendrogram(sch.linkage(sc_X_train, method = 'ward'), labels = sample_list)\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Hierarchical Clustering to the dataset\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "hc = AgglomerativeClustering(n_clusters = 10, affinity = 'euclidean', linkage = 'ward')\n",
    "y_hc = hc.fit_predict(sc_X_train)\n",
    "\n",
    "plt.rc('font', size = 10) #setting the front size in the plot\n",
    "fig, ax = plt.subplots(figsize = (14,10)) #figure size\n",
    "plt.scatter(X_transformed[y_hc == 0, 0], X_transformed[y_hc == 0, 1], s = 300, c = 'red', label = 'Cluster 1', alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_hc == 1, 0], X_transformed[y_hc == 1, 1], s = 300, c = 'blue', label = 'Cluster 2', alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_hc == 2, 0], X_transformed[y_hc == 2, 1], s = 300, c = 'green', label = 'Cluster 3', alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_hc == 3, 0], X_transformed[y_hc == 3, 1], s = 300, c = 'cyan', label = 'Cluster 4', alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_hc == 4, 0], X_transformed[y_hc == 4, 1], s = 300, c = 'silver', label = 'Cluster 5', alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_hc == 5, 0], X_transformed[y_hc == 5, 1], s = 300, c = 'peru', label = 'Cluster 6',alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_hc == 6, 0], X_transformed[y_hc == 6, 1], s = 300, c = 'lawngreen', label = 'Cluster 7', alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_hc == 7, 0], X_transformed[y_hc == 7, 1], s = 300, c = 'lightgreen', label = 'Cluster 8', alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_hc == 8, 0], X_transformed[y_hc == 8, 1], s = 300, c = 'pink', label = 'Cluster 9', alpha = 0.5)\n",
    "plt.scatter(X_transformed[y_hc == 9, 0], X_transformed[y_hc == 9, 1], s = 300, c = 'purple', label = 'Cluster 10', alpha = 0.5)\n",
    "for i, txt in enumerate(sample_list):\n",
    "    ax.annotate(txt, (X_transformed[:,0][i], X_transformed[:,1][i]))\n",
    "plt.legend()\n",
    "#plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Kmeans/KMeans_Rawdata.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the learned Kmeans model on the scaled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros_like(y_kmeans)\n",
    "for i in range(10):\n",
    "    mask = (y_kmeans == i)\n",
    "    labels[mask] = mode(y[mask])[0]\n",
    "\n",
    "acc_score = accuracy_score(y, labels)\n",
    "print('The accuracy score for optimized K-means algorithm {}.'.format(acc_score))\n",
    "\n",
    "mat = confusion_matrix(y, labels)\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.heatmap(mat.T, square = False, annot = True, fmt = 'd', cbar = False, xticklabels = set_sample, yticklabels = set_sample)\n",
    "plt.xlabel('True label')\n",
    "plt.ylabel('Predicted label')\n",
    "#plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Kmeans/ConfusionMatrix_Kmeans_Quantile_NormalizedData.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As the samples from L3_T2 were clustered in 2 clusters without clustering any other samples from different clusters, the confusion matrix count them as correctly classified. As so 90% accuracy was acheived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying PCA on the scaled data to reduce dimenspionality that contains 99.99% variance of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.decomposition import PCA\n",
    "pca = PCA(.9999)\n",
    "#transforming my features to 2 dimension(feature extraction from my orginal feature set)\n",
    "X_trans = pca.fit_transform(sc_X_train)\n",
    "num_components = pca.n_components_\n",
    "print('{} components retain 99.99% variance of the data and shape {}.'.format(num_components, X_trans.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.decomposition import PCA\n",
    "plt.figure(figsize = (14,10))\n",
    "plt.scatter(X_trans[:,0], X_trans[:,1], c = y, s = 50, cmap = 'tab10', alpha = 0.7)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.colorbar()\n",
    "plt.title('Each color represent each sample set')\n",
    "#fig.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Kmeans/Kmeans_Quantile_NormalizedData_PCA_reduction.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Kmeans algorithm to the scaled reduced dimension dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.cluster import KMeans\n",
    "#import matplotlib.pyplot as plt\n",
    "Kmeans = KMeans(n_clusters = 10, init = 'k-means++', max_iter = 300, n_init = 100) #creating the Kmeans object\n",
    "Kmeans.fit(X_trans) #Reduced scaled feature set\n",
    "y_kmeans = Kmeans.fit_predict(X_trans) #fitting the learning model to the data and predicting the clusters for the samples\n",
    "plt.rc('font', size = 10) #setting the front size in the plot\n",
    "fig, ax = plt.subplots(figsize = (14,10)) #figure size\n",
    "plt.scatter(X_trans[y_kmeans == 0, 0], X_trans[y_kmeans == 0, 1], s = 300, c = 'red', label = 'Cluster 1', alpha = 0.5)\n",
    "plt.scatter(X_trans[y_kmeans == 1, 0], X_trans[y_kmeans == 1, 1], s = 300, c = 'blue', label = 'Cluster 2', alpha = 0.5)\n",
    "plt.scatter(X_trans[y_kmeans == 2, 0], X_trans[y_kmeans == 2, 1], s = 300, c = 'green', label = 'Cluster 3', alpha = 0.5)\n",
    "plt.scatter(X_trans[y_kmeans == 3, 0], X_trans[y_kmeans == 3, 1], s = 300, c = 'cyan', label = 'Cluster 4', alpha = 0.5)\n",
    "plt.scatter(X_trans[y_kmeans == 4, 0], X_trans[y_kmeans == 4, 1], s = 300, c = 'silver', label = 'Cluster 5', alpha = 0.5)\n",
    "plt.scatter(X_trans[y_kmeans == 5, 0], X_trans[y_kmeans == 5, 1], s = 300, c = 'peru', label = 'Cluster 6',alpha = 0.5)\n",
    "plt.scatter(X_trans[y_kmeans == 6, 0], X_trans[y_kmeans == 6, 1], s = 300, c = 'lawngreen', label = 'Cluster 7', alpha = 0.5)\n",
    "plt.scatter(X_trans[y_kmeans == 7, 0], X_trans[y_kmeans == 7, 1], s = 300, c = 'lightgreen', label = 'Cluster 8', alpha = 0.5)\n",
    "plt.scatter(X_trans[y_kmeans == 8, 0], X_trans[y_kmeans == 8, 1], s = 300, c = 'pink', label = 'Cluster 9', alpha = 0.5)\n",
    "plt.scatter(X_trans[y_kmeans == 9, 0], X_trans[y_kmeans == 9, 1], s = 300, c = 'purple', label = 'Cluster 10', alpha = 0.5)\n",
    "plt.scatter(Kmeans.cluster_centers_[:, 0], Kmeans.cluster_centers_[:, 1], s = 100, marker = '+', c = 'black', label = 'Centroids', alpha = 0.5)\n",
    "\n",
    "for i, txt in enumerate(sample_list):\n",
    "    ax.annotate(txt, (X_trans[:,0][i], X_trans[:,1][i]))\n",
    "plt.legend()\n",
    "#fig.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Kmeans/Kmeans_Quantile_NormalizedData_PCA_reduced.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the learned Kmeans model on the scaled reduced dimension dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros_like(y_kmeans)\n",
    "for i in range(10):\n",
    "    mask = (y_kmeans == i)\n",
    "    labels[mask] = mode(y[mask])[0]\n",
    "\n",
    "acc_score = accuracy_score(y, labels)\n",
    "print('The accuracy score for optimized K-means algorithm {}.'.format(acc_score))\n",
    "\n",
    "mat = confusion_matrix(y, labels)\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.heatmap(mat.T, square = False, annot = True, fmt = 'd', cbar = False, xticklabels = set_sample, yticklabels = set_sample)\n",
    "plt.xlabel('True label')\n",
    "plt.ylabel('Predicted label')\n",
    "#plt.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Kmeans/ConfusionMatrix_Kmeans_Quantile_NormalizedData_PCA_reduced.png', dpi = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral clustering performed on reduced dimension scaled data, but Spectraal clustering performs poorly on classic PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "SCluster = SpectralClustering(n_clusters = 10, n_init = 100, affinity = 'rbf', assign_labels = 'kmeans')\n",
    "y_SC = SCluster.fit_predict(X_trans)\n",
    "\n",
    "plt.rc('font', size = 10) #setting the front size in the plot\n",
    "fig, ax = plt.subplots(figsize = (14,10)) #figure size\n",
    "plt.scatter(X_trans[:, 0], X_trans[:, 1], c = y_SC, s = 300, cmap='tab10')\n",
    "#plt.title('Accuracy score {} for {} PCA components.'.format(acc_score, 5))\n",
    "for i, txt in enumerate(sample_list):\n",
    "    ax.annotate(txt, (X_trans[:,0][i], X_trans[:,1][i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of the oaptimized Kmeans model on different number of Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components = [*range(num_components - 15, num_components + 1)]\n",
    "\n",
    "fig, ax = plt.subplots(5, 3, figsize=(14, 10))\n",
    "ax = np.ravel(ax)\n",
    "fig.subplots_adjust(hspace = 0.01, wspace = 0.1)\n",
    "for i in range(15):\n",
    "    Kmeans_opt = KMeans(n_clusters = 10, init = 'k-means++', max_iter = 1000, n_init = 300)\n",
    "    pca_cps = PCA(n_components = pca_components[i])\n",
    "    X_pca = pca_cps.fit_transform(qt_X_train)\n",
    "    Kmeans_opt.fit(X_pca)\n",
    "    y_kmeans_opt = Kmeans_opt.fit_predict(X_pca)\n",
    "    labels = np.zeros_like(y_kmeans_opt)\n",
    "    for j in range(10):\n",
    "        mask = (y_kmeans_opt == j)\n",
    "        labels[mask] = mode(y[mask])[0]\n",
    "    acc_score = accuracy_score(y, labels)\n",
    "    ax[i].scatter(X_pca[:, 0], X_pca[:, 1], c = y_kmeans_opt, s = 5, cmap='tab10')\n",
    "    ax[i].title.set_text('Accuracy score {} for {} PCA components.'.format(acc_score, pca_components[i]))\n",
    "plt.tight_layout()\n",
    "fig.savefig('C:/Users/Tamal/Documents/Thesis Files/Images/Kmeans/different_PCA_components_Quantile_Normalized.png', dpi = 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
